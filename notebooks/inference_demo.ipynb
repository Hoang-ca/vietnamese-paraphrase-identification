{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üáªüá≥ Vietnamese Paraphrase Identification ‚Äî Inference Demo\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vmhdaica/vietnamese-paraphrase-identification/blob/main/notebooks/inference_demo.ipynb)\n",
                "\n",
                "This notebook loads the fine-tuned **PhoBERT-base-v2** model from HuggingFace Hub and lets you:\n",
                "1. Run **batch predictions** on sample sentence pairs\n",
                "2. Launch an **interactive Gradio UI** right inside Colab\n",
                "\n",
                "**Model:** [`vmhdaica/vnpi_model_checkpoint_3135`](https://huggingface.co/vmhdaica/vnpi_model_checkpoint_3135)  \n",
                "**Accuracy:** 97.02% | **Macro-F1:** 0.876 | **PR-AUC:** 0.9995\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip -q install transformers torch gradio"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model from HuggingFace Hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
                "\n",
                "MODEL_ID = \"vmhdaica/vnpi_model_checkpoint_3135\"\n",
                "MAX_LENGTH = 256\n",
                "\n",
                "print(f\"Loading model: {MODEL_ID}\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)\n",
                "\n",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "model = model.to(device).eval()\n",
                "print(f\"‚úÖ Model loaded on {device}\")\n",
                "print(f\"   Labels: {model.config.id2label}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prediction Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_pair(s1: str, s2: str) -> dict:\n",
                "    \"\"\"Predict whether two Vietnamese sentences are paraphrases.\"\"\"\n",
                "    inputs = tokenizer(\n",
                "        s1, s2,\n",
                "        truncation=True, max_length=MAX_LENGTH,\n",
                "        return_tensors=\"pt\",\n",
                "    ).to(device)\n",
                "    inputs.pop(\"token_type_ids\", None)  # PhoBERT/RoBERTa doesn't use this\n",
                "\n",
                "    with torch.no_grad():\n",
                "        logits = model(**inputs).logits\n",
                "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
                "\n",
                "    label = \"‚úÖ Paraphrase\" if probs[1] > probs[0] else \"‚ùå Not paraphrase\"\n",
                "    return {\n",
                "        \"label\": label,\n",
                "        \"p_paraphrase\": round(float(probs[1]), 4),\n",
                "        \"p_not_paraphrase\": round(float(probs[0]), 4),\n",
                "    }\n",
                "\n",
                "# Quick test\n",
                "result = predict_pair(\"H√¥m nay tr·ªùi m∆∞a r·∫•t to.\", \"Th·ªùi ti·∫øt h√¥m nay m∆∞a l·ªõn.\")\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Batch Predictions ‚Äî Sample Pairs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "test_pairs = [\n",
                "    (\"H√¥m nay tr·ªùi m∆∞a r·∫•t to.\",\n",
                "     \"Th·ªùi ti·∫øt h√¥m nay m∆∞a l·ªõn.\"),\n",
                "    (\"Gi√° v√†ng tƒÉng m·∫°nh.\",\n",
                "     \"Tr·∫≠n ƒë·∫•u t·ªëi qua r·∫•t h·∫•p d·∫´n.\"),\n",
                "    (\"Th·ªß t∆∞·ªõng ƒë√£ h·ªçp v·ªõi c√°c b·ªô tr∆∞·ªüng.\",\n",
                "     \"Cu·ªôc h·ªçp c·ªßa Th·ªß t∆∞·ªõng v·ªõi n·ªôi c√°c ƒë√£ di·ªÖn ra.\"),\n",
                "    (\"H√† N·ªôi l√† th·ªß ƒë√¥ c·ªßa Vi·ªát Nam.\",\n",
                "     \"TP.HCM l√† th√†nh ph·ªë l·ªõn nh·∫•t Vi·ªát Nam.\"),\n",
                "    (\"C√¥ ·∫•y r·∫•t gi·ªèi ti·∫øng Anh.\",\n",
                "     \"Kh·∫£ nƒÉng ti·∫øng Anh c·ªßa c√¥ ·∫•y r·∫•t t·ªët.\"),\n",
                "    (\"T√¥i ƒëi ƒÉn ph·ªü s√°ng nay.\",\n",
                "     \"S√°ng nay t√¥i ƒë√£ th∆∞·ªüng th·ª©c m·ªôt t√¥ ph·ªü.\"),\n",
                "    (\"Vi·ªát Nam n·∫±m ·ªü ƒê√¥ng Nam √Å.\",\n",
                "     \"ƒê·∫•t n∆∞·ªõc Vi·ªát Nam thu·ªôc khu v·ª±c ƒê√¥ng Nam √Å.\"),\n",
                "    (\"Con m√®o ng·ªìi tr√™n b√†n.\",\n",
                "     \"Chi·∫øc xe ƒëang ch·∫°y tr√™n ƒë∆∞·ªùng.\"),\n",
                "]\n",
                "\n",
                "results = []\n",
                "for s1, s2 in test_pairs:\n",
                "    r = predict_pair(s1, s2)\n",
                "    results.append({\n",
                "        \"Sentence 1\": s1,\n",
                "        \"Sentence 2\": s2,\n",
                "        \"Prediction\": r[\"label\"],\n",
                "        \"P(paraphrase)\": r[\"p_paraphrase\"],\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "df.style.set_properties(**{\"text-align\": \"left\"})"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Try Your Own Sentences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ‚úèÔ∏è Change these sentences and re-run this cell!\n",
                "\n",
                "sentence_1 = \"Vi·ªát Nam c√≥ nhi·ªÅu c·∫£nh ƒë·∫πp.\"\n",
                "sentence_2 = \"ƒê·∫•t n∆∞·ªõc Vi·ªát Nam r·∫•t nhi·ªÅu phong c·∫£nh tuy·ªát v·ªùi.\"\n",
                "\n",
                "result = predict_pair(sentence_1, sentence_2)\n",
                "print(f\"  Sentence 1: {sentence_1}\")\n",
                "print(f\"  Sentence 2: {sentence_2}\")\n",
                "print(f\"  ‚Üí {result['label']}  (confidence: {result['p_paraphrase']:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. üé® Interactive Gradio Demo\n",
                "\n",
                "Run this cell to launch a **live UI** directly inside Colab!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import gradio as gr\n",
                "\n",
                "def gradio_predict(s1: str, s2: str) -> dict:\n",
                "    if not s1.strip() or not s2.strip():\n",
                "        return {\"paraphrase\": 0.0, \"not_paraphrase\": 1.0}\n",
                "    inputs = tokenizer(s1, s2, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\").to(device)\n",
                "    inputs.pop(\"token_type_ids\", None)\n",
                "    with torch.no_grad():\n",
                "        probs = torch.softmax(model(**inputs).logits, dim=-1)[0].cpu().numpy()\n",
                "    return {\"paraphrase\": float(probs[1]), \"not_paraphrase\": float(probs[0])}\n",
                "\n",
                "examples = [\n",
                "    [\"H√¥m nay tr·ªùi m∆∞a r·∫•t to.\", \"Th·ªùi ti·∫øt h√¥m nay m∆∞a l·ªõn.\"],\n",
                "    [\"Gi√° v√†ng tƒÉng m·∫°nh.\", \"Tr·∫≠n ƒë·∫•u t·ªëi qua r·∫•t h·∫•p d·∫´n.\"],\n",
                "    [\"Th·ªß t∆∞·ªõng ƒë√£ h·ªçp v·ªõi c√°c b·ªô tr∆∞·ªüng.\", \"Cu·ªôc h·ªçp c·ªßa Th·ªß t∆∞·ªõng v·ªõi n·ªôi c√°c ƒë√£ di·ªÖn ra.\"],\n",
                "    [\"H√† N·ªôi l√† th·ªß ƒë√¥ c·ªßa Vi·ªát Nam.\", \"TP.HCM l√† th√†nh ph·ªë l·ªõn nh·∫•t Vi·ªát Nam.\"],\n",
                "    [\"C√¥ ·∫•y r·∫•t gi·ªèi ti·∫øng Anh.\", \"Kh·∫£ nƒÉng ti·∫øng Anh c·ªßa c√¥ ·∫•y r·∫•t t·ªët.\"],\n",
                "    [\"T√¥i ƒëi ƒÉn ph·ªü s√°ng nay.\", \"S√°ng nay t√¥i ƒë√£ th∆∞·ªüng th·ª©c m·ªôt t√¥ ph·ªü.\"],\n",
                "]\n",
                "\n",
                "demo = gr.Interface(\n",
                "    fn=gradio_predict,\n",
                "    inputs=[\n",
                "        gr.Textbox(label=\"C√¢u 1 / Sentence 1\", placeholder=\"Nh·∫≠p c√¢u ti·∫øng Vi·ªát...\", lines=2),\n",
                "        gr.Textbox(label=\"C√¢u 2 / Sentence 2\", placeholder=\"Nh·∫≠p c√¢u ti·∫øng Vi·ªát...\", lines=2),\n",
                "    ],\n",
                "    outputs=gr.Label(label=\"Result\", num_top_classes=2),\n",
                "    title=\"üáªüá≥ Vietnamese Paraphrase Identification\",\n",
                "    description=\"Compare two Vietnamese sentences. Model: PhoBERT-base-v2 fine-tuned on 40K+ pairs ‚Äî 97.02% accuracy.\",\n",
                "    examples=examples,\n",
                "    theme=gr.themes.Soft(),\n",
                ")\n",
                "\n",
                "demo.launch(share=True)  # share=True creates a public URL"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "**Resources:**\n",
                "- ü§ó Model: [`vmhdaica/vnpi_model_checkpoint_3135`](https://huggingface.co/vmhdaica/vnpi_model_checkpoint_3135)\n",
                "- üìÇ GitHub: [`vietnamese-paraphrase-identification`](https://github.com/Hoang-ca/vietnamese-paraphrase-identification)\n",
                "- üìù Model Card: see `MODEL_CARD.md` in the repo"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        },
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "accelerator": "GPU"
    },
    "nbformat": 4,
    "nbformat_minor": 4
}